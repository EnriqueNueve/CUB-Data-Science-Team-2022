{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb7017d",
   "metadata": {},
   "source": [
    "# Train model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b965c66",
   "metadata": {},
   "source": [
    "## Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b81462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3de906",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd87444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding function\n",
    "def parse_record(record):\n",
    "    name_to_features = {\n",
    "        'features': tf.io.FixedLenFeature([95], tf.float32),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "def decode_record(record):\n",
    "    features = record['features']\n",
    "    target = record['label']\n",
    "    return (features,target)\n",
    "\n",
    "def prepData(record):\n",
    "    record = parse_record(record)\n",
    "    X,y = decode_record(record)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3843417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\"\"\"\n",
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.9, val_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/rick/Desktop/Boulder/Spring2022/data_science/workspace/CUB-Data-Science-Team-2022/ml_dev_tutorial/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = data_path+\"/tfRecord/train.tfrecord\"\n",
    "train_dataset = tf.data.TFRecordDataset(train_file_name)\n",
    "train_dataset = train_dataset.map(prepData, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(10000)\n",
    "train_dataset = train_dataset.batch(64)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Split train data into train and val \n",
    "train_size = sum(1 for _ in train_dataset)\n",
    "train_dataset, val_dataset = get_dataset_partitions_tf(train_dataset,train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = data_path+\"/tfRecord/test.tfrecord\"\n",
    "test_dataset = tf.data.TFRecordDataset(test_file_name)\n",
    "test_dataset = test_dataset.map(prepData, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.shuffle(10000)\n",
    "test_dataset = test_dataset.batch(64)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train data is feeding right \n",
    "for t in train_dataset.take(1):\n",
    "    X, y  = t\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "# Test test data is feeding right\n",
    "for t in test_dataset.take(1):\n",
    "    X, y  = t\n",
    "    print(X.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d4ce3",
   "metadata": {},
   "source": [
    "## Declare model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72329c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1_ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(L1_ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(tf.math.abs(inputs)))\n",
    "        return inputs\n",
    "\n",
    "class customDenseLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    y = eLU(w.x+b)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,units):\n",
    "        super(customDenseLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.r = L1_ActivityRegularization(1e-4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"w\"\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True,name=\"b\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.r(tf.nn.elu(tf.matmul(inputs, self.w) + self.b))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankModel(Model):\n",
    "    def __init__(self,):\n",
    "        super(BankModel, self).__init__()\n",
    "        \n",
    "        # Declare model layers \n",
    "        self.layer_1 = customDenseLayer(48)\n",
    "        self.layer_2 = tf.keras.layers.Dropout(.3)\n",
    "        self.layer_3 = layers.Dense(\n",
    "                units=16,\n",
    "                activation=\"elu\",\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3),\n",
    "                bias_regularizer=regularizers.l2(1e-3),\n",
    "                activity_regularizer=regularizers.l2(1e-3)\n",
    "            )\n",
    "        self.layer_4 = tf.keras.layers.Dropout(.3)\n",
    "        self.layer_5 = layers.Dense(\n",
    "                units=1,\n",
    "                activation=\"sigmoid\",\n",
    "            )\n",
    "        \n",
    "        # Declare loss and metrics\n",
    "        self.loss_cc = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.tp = tf.keras.metrics.TruePositives(name='tp')\n",
    "        self.fp = tf.keras.metrics.FalsePositives(name='fp')\n",
    "        self.tn = tf.keras.metrics.TrueNegatives(name='tn')\n",
    "        self.fn = tf.keras.metrics.FalseNegatives(name='fn') \n",
    "        self.acc = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "        self.prec = tf.keras.metrics.Precision(name='precision')\n",
    "        self.rec = tf.keras.metrics.Recall(name='recall')\n",
    "        self.auc = tf.keras.metrics.AUC(name='auc')        \n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"List of the model's metrics.\n",
    "        We make sure the loss tracker is listed as part of `model.metrics`\n",
    "        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker\n",
    "        at the start of each epoch and at the start of an `evaluate()` call.\n",
    "        \"\"\"\n",
    "        return [self.loss_tracker,self.acc,self.rec,self.prec,self.auc,\\\n",
    "                        self.tp,self.fp,self.tn,self.fn]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.layer_1(inputs)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        yh = self.layer_5(x)\n",
    "        return yh\n",
    "\n",
    "    def train_step(self, data):\n",
    "        X, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = self.layer_1(X)\n",
    "            x = self.layer_2(x)\n",
    "            x = self.layer_3(x)\n",
    "            x = self.layer_4(x)\n",
    "            yh = self.layer_5(x)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.loss_cc(y,yh)\n",
    "            self.loss_tracker.update_state(loss)\n",
    "            \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.acc.update_state(y,tf.math.round(yh))\n",
    "        self.rec.update_state(y,yh)\n",
    "        self.prec.update_state(y,yh)\n",
    "        self.auc.update_state(y,yh)\n",
    "        self.tp.update_state(y,yh)\n",
    "        self.fp.update_state(y,yh)\n",
    "        self.tn.update_state(y,yh)\n",
    "        self.fn.update_state(y,yh)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value   \n",
    "        results = {\"loss\": self.loss_tracker.result()}\n",
    "        results[\"acc\"] = self.acc.result()\n",
    "        results[\"recall\"] = self.rec.result()\n",
    "        results[\"precision\"] = self.prec.result()\n",
    "        results[\"auc\"] = self.auc.result()\n",
    "        results[\"tp\"] = self.tp.result()\n",
    "        results[\"fp\"] = self.fp.result()\n",
    "        results[\"tn\"] = self.tn.result()\n",
    "        results[\"fn\"] = self.fn.result()\n",
    "        return results\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        X, y = data\n",
    "        \n",
    "        x = self.layer_1(X)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        yh = self.layer_5(x)\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = self.loss_cc(y,yh)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.acc.update_state(y,tf.math.round(yh))\n",
    "        self.rec.update_state(y,yh)\n",
    "        self.prec.update_state(y,yh)\n",
    "        self.auc.update_state(y,yh)\n",
    "        self.tp.update_state(y,yh)\n",
    "        self.fp.update_state(y,yh)\n",
    "        self.tn.update_state(y,yh)\n",
    "        self.fn.update_state(y,yh)\n",
    "        \n",
    "        # Return a dict mapping metric names to current value \n",
    "        results = {\"loss\": self.loss_tracker.result()}\n",
    "        results[\"acc\"] = self.acc.result()\n",
    "        results[\"recall\"] = self.rec.result()\n",
    "        results[\"precision\"] = self.prec.result()\n",
    "        results[\"auc\"] = self.auc.result()\n",
    "        results[\"tp\"] = self.tp.result()\n",
    "        results[\"fp\"] = self.fp.result()\n",
    "        results[\"tn\"] = self.tn.result()\n",
    "        results[\"fn\"] = self.fn.result()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def build_graph(self, raw_shape):\n",
    "        x = tf.keras.layers.Input(shape=raw_shape)\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30579ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BankModel()\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "#model.build_graph(input_shape).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcf578",
   "metadata": {},
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=75, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9af36",
   "metadata": {},
   "source": [
    "## Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61243025",
   "metadata": {},
   "source": [
    "### Reload test data to convert to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54308a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = data_path+\"/tfRecord/test.tfrecord\"\n",
    "test_dataset = tf.data.TFRecordDataset(test_file_name)\n",
    "test_dataset = test_dataset.map(prepData, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.cache()\n",
    "test_dataset = test_dataset.shuffle(10000)\n",
    "test_dataset = test_dataset.batch(3000)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_dataset)\n",
    "next_element = iterator.get_next()\n",
    "X_test, y_test = next_element\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996724d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, verbose=1)\n",
    "y_pred = np.round(Y_pred).flatten()\n",
    "target_names = ['no fraud','fraud']\n",
    "\n",
    "print(classification_report(y_test, y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = pd.DataFrame(cm, range(2),range(2))\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8426147",
   "metadata": {},
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67563c",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = BankModel()\n",
    "new_model.build((None, 95))\n",
    "new_model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412615c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f58ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94268a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea55b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cub_data_science",
   "language": "python",
   "name": "cub_data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
